{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzvKMmL6zsNx",
        "outputId": "e790c2ab-fe61-4daf-eff0-fdfacda724a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 22:53:51--  https://github.com/Mozilla-Ocho/llamafile/releases/download/0.9.2/llamafile-0.9.2\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/689773665/bce901fb-cfb5-4195-8fde-78a914b81a6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250416T225351Z&X-Amz-Expires=300&X-Amz-Signature=cb1b46ce330524f4e47857af319b6c40a3c91c0cf7b6cbc7a3a5e4f2e3864840&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.9.2&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-04-16 22:53:51--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/689773665/bce901fb-cfb5-4195-8fde-78a914b81a6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250416T225351Z&X-Amz-Expires=300&X-Amz-Signature=cb1b46ce330524f4e47857af319b6c40a3c91c0cf7b6cbc7a3a5e4f2e3864840&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.9.2&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 307675955 (293M) [application/octet-stream]\n",
            "Saving to: ‘llamafile-0.9.2’\n",
            "\n",
            "llamafile-0.9.2     100%[===================>] 293.42M  77.2MB/s    in 3.6s    \n",
            "\n",
            "2025-04-16 22:53:55 (82.6 MB/s) - ‘llamafile-0.9.2’ saved [307675955/307675955]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/Mozilla-Ocho/llamafile/releases/download/0.9.2/llamafile-0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llamafile-0.9.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFo21jeKzs72",
        "outputId": "d8907932-1c05-42d0-d47b-edc20393a3b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement llamafile-0.9.2 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for llamafile-0.9.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Et-HFuIbzy0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!chmod +x /content/llamafile-0.9.2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gYKAMxdP0Vjl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "source": [
        "!./content/llamafile-0.9.2"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1nxLxmP0Wgc",
        "outputId": "b0208dbb-8e3a-4b88-f95c-e37f08e9b821"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./content/llamafile-0.9.2: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!chmod +x /content/llamafile-0.9.2\n",
        "!./content/llamafile-0.9.2 --help"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejUx9X7n0ZIs",
        "outputId": "dc5a1dad-e70b-4220-ecf6-3cc5097f6b8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./content/llamafile-0.9.2: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!wget https://github.com/Mozilla-Ocho/llamafile/releases/download/0.9.2/llamafile-0.9.2"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L3Rdp_b0gDE",
        "outputId": "faf6f37a-3d99-4224-a592-cbf86ad73289"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 22:57:15--  https://github.com/Mozilla-Ocho/llamafile/releases/download/0.9.2/llamafile-0.9.2\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/689773665/bce901fb-cfb5-4195-8fde-78a914b81a6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250416T225715Z&X-Amz-Expires=300&X-Amz-Signature=aa55c4a9d944ce6071131229c914cf04975dc51d1d82b7b739ad9cafc9611766&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.9.2&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-04-16 22:57:15--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/689773665/bce901fb-cfb5-4195-8fde-78a914b81a6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250416T225715Z&X-Amz-Expires=300&X-Amz-Signature=aa55c4a9d944ce6071131229c914cf04975dc51d1d82b7b739ad9cafc9611766&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.9.2&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 307675955 (293M) [application/octet-stream]\n",
            "Saving to: ‘llamafile-0.9.2.1’\n",
            "\n",
            "llamafile-0.9.2.1   100%[===================>] 293.42M  12.6MB/s    in 4.2s    \n",
            "\n",
            "2025-04-16 22:57:19 (69.7 MB/s) - ‘llamafile-0.9.2.1’ saved [307675955/307675955]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!chmod +x /content/llamafile-0.9.2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hBYFft-O0jOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!chmod +x /content/llamafile-0.9.2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OnsJqyZH0jPC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "source": [
        "!./llamafile-0.9.2 --help"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jZUnh-470kuk",
        "outputId": "bf35c23d-e16f-47a7-a50f-ef7eda913cf3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b7\u001b[?47h\u001b[?1h\u001b=\r\u001b[4mLLAMAFILE\u001b[24m(1)                General Commands Manual               \u001b[4mLLAMAFILE\u001b[24m(1)\u001b[m\r\n",
            "\u001b[m\r\n",
            "\u001b[1mNAME\u001b[0m\u001b[m\r\n",
            "       llamafile — large language model runner\u001b[m\r\n",
            "\u001b[m\r\n",
            "\u001b[1mSYNOPSIS\u001b[0m\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--chat\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[0m\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--server\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m [\u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m]\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m \u001b[1m--image\u001b[0m\u001b[m\r\n",
            "                 \u001b[4mgraphic.png\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\r\n",
            "\u001b[m\r\n",
            "\u001b[1mDESCRIPTION\u001b[0m\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22mis a large language model tool. It has use cases such as:\u001b[m\r\n",
            "\u001b[m\r\n",
            "       \u001b[1m-   \u001b[22mCode completion\u001b[m\r\n",
            "       \u001b[1m-   \u001b[22mProse composition\u001b[m\r\n",
            "       \u001b[1m-   \u001b[22mChatbot that passes the Turing test\u001b[m\r\n",
            "       \u001b[1m-   \u001b[22mText/image summarization and analysis\u001b[m\r\n",
            "\u001b[m\r\n",
            "\u001b[1mMODES\u001b[0m\u001b[m\r\n",
            "       There's three modes of operation: \u001b[1m--chat\u001b[22m, \u001b[1m--server\u001b[22m, and \u001b[1m--cli\u001b[22m.  If none\u001b[m\r\n",
            "       of  these flags is specified, then llamafile makes its best guess about\u001b[m\n",
            "\u001b[7m/tmp/paginate.p9ka9n\u001b[m\u001b[K\u0007\u001b[H\u001b[2J\u001b[H\u001b[H\u001b[2J\u001b[H\u001b[4mLLAMAFILE\u001b[24m(1)                General Commands Manual               \u001b[4mLLAMAFILE\u001b[24m(1)\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mNAME\u001b[0m\u001b[m\n",
            "       llamafile — large language model runner\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mSYNOPSIS\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--chat\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--server\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m [\u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m]\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m \u001b[1m--image\u001b[0m\u001b[m\n",
            "                 \u001b[4mgraphic.png\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mDESCRIPTION\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22mis a large language model tool. It has use cases such as:\u001b[m\n",
            "\u001b[m\n",
            "       \u001b[1m-   \u001b[22mCode completion\u001b[m\n",
            "       \u001b[1m-   \u001b[22mProse composition\u001b[m\n",
            "       \u001b[1m-   \u001b[22mChatbot that passes the Turing test\u001b[m\n",
            "       \u001b[1m-   \u001b[22mText/image summarization and analysis\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mMODES\u001b[0m\u001b[m\n",
            "       There's three modes of operation: \u001b[1m--chat\u001b[22m, \u001b[1m--server\u001b[22m, and \u001b[1m--cli\u001b[22m.  If none\u001b[m\n",
            "       of  these flags is specified, then llamafile makes its best guess about\u001b[m\n",
            ":\u001b[K\u0007\u001b[H\u001b[2J\u001b[H\u001b[H\u001b[2J\u001b[H\u001b[4mLLAMAFILE\u001b[24m(1)                General Commands Manual               \u001b[4mLLAMAFILE\u001b[24m(1)\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mNAME\u001b[0m\u001b[m\n",
            "       llamafile — large language model runner\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mSYNOPSIS\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--chat\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--server\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m [\u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m]\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m \u001b[1m--image\u001b[0m\u001b[m\n",
            "                 \u001b[4mgraphic.png\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mDESCRIPTION\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22mis a large language model tool. It has use cases such as:\u001b[m\n",
            "\u001b[m\n",
            "       \u001b[1m-   \u001b[22mCode completion\u001b[m\n",
            "       \u001b[1m-   \u001b[22mProse composition\u001b[m\n",
            "       \u001b[1m-   \u001b[22mChatbot that passes the Turing test\u001b[m\n",
            "       \u001b[1m-   \u001b[22mText/image summarization and analysis\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mMODES\u001b[0m\u001b[m\n",
            "       There's three modes of operation: \u001b[1m--chat\u001b[22m, \u001b[1m--server\u001b[22m, and \u001b[1m--cli\u001b[22m.  If none\u001b[m\n",
            "       of  these flags is specified, then llamafile makes its best guess about\u001b[m\n",
            ":\u001b[K"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d1de2e811f7e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./llamafile-0.9.2 --help'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llamafile-0.9.2 --"
      ],
      "metadata": {
        "id": "vInUJ_D00rOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-IQ3_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMxFQHy900tE",
        "outputId": "70996605-d888-42c3-9251-2338ac4f3647"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 22:59:19--  https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-IQ3_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.168.73.106, 3.168.73.129, 3.168.73.38, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.168.73.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457ed2dc07e76a1bde8d6/6d98e192e0c830d32a001c71ac58d103d67dbe66347c67c75129d99c66e265ee?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250416T225919Z&X-Amz-Expires=3600&X-Amz-Signature=a86c77962cb71cc57ae1e0b958fd8a78928cd4f0073d0d2512282d3fac2d9acb&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct-IQ3_M.gguf%3B+filename%3D%22Llama-3.2-1B-Instruct-IQ3_M.gguf%22%3B&x-id=GetObject&Expires=1744847959&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDg0Nzk1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdlZDJkYzA3ZTc2YTFiZGU4ZDYvNmQ5OGUxOTJlMGM4MzBkMzJhMDAxYzcxYWM1OGQxMDNkNjdkYmU2NjM0N2M2N2M3NTEyOWQ5OWM2NmUyNjVlZSoifV19&Signature=YEgyxZoyPanlfSkM%7ESL9oh%7EVFyN4Z5cX%7EuAVP5KATtitqZylP4tnliSgc5kgG3477eBLCsn0k8Nzw1OExz0FEWzCb9FQhPAR6YaYpaWYYeiSBHXCDH8LIX-JH2JAWe7hczP%7E3AIuDc7%7EvjdDWWy0A0W7rVioh%7EGt9OYWLPSa%7EulxqqeOEodFrPIfxSzNDLTxgorvWpksEdcz132Drtc2Amp5m61hs%7ELIarmZnsYoR2K6oFE6dKgPjTlr9ly8uOZ4FtRuJmv4uGXqtfjnWAEF-NR3yvkMwi6WWjWtBc14q9eqoiFn2y8JS5nZixDQ8D69QIlxHwskSEajjVvV6jOpcg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-04-16 22:59:19--  https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457ed2dc07e76a1bde8d6/6d98e192e0c830d32a001c71ac58d103d67dbe66347c67c75129d99c66e265ee?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250416T225919Z&X-Amz-Expires=3600&X-Amz-Signature=a86c77962cb71cc57ae1e0b958fd8a78928cd4f0073d0d2512282d3fac2d9acb&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct-IQ3_M.gguf%3B+filename%3D%22Llama-3.2-1B-Instruct-IQ3_M.gguf%22%3B&x-id=GetObject&Expires=1744847959&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDg0Nzk1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdlZDJkYzA3ZTc2YTFiZGU4ZDYvNmQ5OGUxOTJlMGM4MzBkMzJhMDAxYzcxYWM1OGQxMDNkNjdkYmU2NjM0N2M2N2M3NTEyOWQ5OWM2NmUyNjVlZSoifV19&Signature=YEgyxZoyPanlfSkM%7ESL9oh%7EVFyN4Z5cX%7EuAVP5KATtitqZylP4tnliSgc5kgG3477eBLCsn0k8Nzw1OExz0FEWzCb9FQhPAR6YaYpaWYYeiSBHXCDH8LIX-JH2JAWe7hczP%7E3AIuDc7%7EvjdDWWy0A0W7rVioh%7EGt9OYWLPSa%7EulxqqeOEodFrPIfxSzNDLTxgorvWpksEdcz132Drtc2Amp5m61hs%7ELIarmZnsYoR2K6oFE6dKgPjTlr9ly8uOZ4FtRuJmv4uGXqtfjnWAEF-NR3yvkMwi6WWjWtBc14q9eqoiFn2y8JS5nZixDQ8D69QIlxHwskSEajjVvV6jOpcg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.33.252.17, 13.33.252.46, 13.33.252.50, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.33.252.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 657289344 (627M)\n",
            "Saving to: ‘Llama-3.2-1B-Instruct-IQ3_M.gguf’\n",
            "\n",
            "Llama-3.2-1B-Instru 100%[===================>] 626.84M  60.5MB/s    in 11s     \n",
            "\n",
            "2025-04-16 22:59:30 (58.6 MB/s) - ‘Llama-3.2-1B-Instruct-IQ3_M.gguf’ saved [657289344/657289344]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JcEFkBIv1Hgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### aشغال"
      ],
      "metadata": {
        "id": "q-D8edPt1mPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./llamafile-0.9.2  -m /content/Llama-3.2-1B-Instruct-IQ3_M.gguf -p hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uGAoNh81AQe",
        "outputId": "5bb63303-45c6-4c58-9eca-7fef4f71e19d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "note: if you have an AMD or NVIDIA GPU then you need to pass -ngl 9999 to enable GPU offloading\n",
            "Log start\n",
            "main: build = 1500 (a30b324)\n",
            "main: built with cosmocc (GCC) 11.2.0 for x86_64-linux-cosmo\n",
            "main: seed  = 1744844496\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 147 tensors from /content/Llama-3.2-1B-Instruct-IQ3_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 16\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 27\n",
            "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-1B-Instruct-GGU...\n",
            "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 112\n",
            "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   34 tensors\n",
            "llama_model_loader: - type q4_K:   34 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llama_model_loader: - type iq3_s:   78 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_layer          = 16\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 512\n",
            "llm_load_print_meta: n_embd_v_gqa     = 512\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = IQ3_S mix - 3.66 bpw\n",
            "llm_load_print_meta: model params     = 1.24 B\n",
            "llm_load_print_meta: model size       = 619.37 MiB (4.20 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 1B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.08 MiB\n",
            "\u001b[Kllm_load_tensors:        CPU buffer size =   619.37 MiB\n",
            ".....................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 2048\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   544.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 518\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 1 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "\n",
            "hi, I wanted to reach out and ask a question about the book \"The Perks of Being a Wallflower\" by Stephen E. Miller. I recently finished reading the book and it really stood out to me.\n",
            "\n",
            "As I was reading the book, I became fascinated by the main character Charlie's experiences as a freshman at a traditional high school. I found myself identifying with his struggles and insecurities, and the way he navigated social situations with such vulnerability and humor. I loved how he used humor to cope with his emotions and make social interactions more approachable.\n",
            "\n",
            "I was particularly struck by his relationship with Patrick, the charismatic and outgoing senior. Their dynamic was so intriguing, and I loved how Patrick's character represented the \"outsider\" that Charlie felt he was.\n",
            "\n",
            "I also noticed how the book portrayed mental health struggles, anxiety, and depression as something that Charlie and Patrick both faced, yet often struggled to acknowledge or discuss. It was remarkable to see how the characters navigated these issues, often using humor and self-deprecation as coping mechanisms.\n",
            "\n",
            "One question I had is what Charlie's character development was like throughout the book? Was there a turning point or a particular moment when he changed from an outsider to an accepted member of the group? I wanted to know what drove him to be a part of the group, and how did he navigate the pressures and expectations that came with it.\n",
            "\n",
            "Lastly, I was surprised by how the book portrayed the consequences of the characters' actions, as well as the impact it had on their mental health. It was both heartbreaking and thought-provoking.\n",
            "\n",
            "Thank you for listening to my question, and I look forward to hearing your insights on Charlie's character development!\n",
            "\n",
            "(Note: I couldn't find any information on the author's name being \"Stephen E. Miller\" - it's possible this is a misquote or a variation, or perhaps it's a personalization of the book?)\n",
            "\n",
            "Edit: I also have to correct myself - my actual question is \"The Perks of Being a Wallflower\" by Stephen Chbosky, not by a fictional Stephen E. Miller. I appreciate your help in clarifying the correct book title. Thank you again for your response!) [end of text]\n",
            "\n",
            "\n",
            "llama_print_timings:        load time =     825.40 ms\n",
            "llama_print_timings:      sample time =      78.01 ms /   442 runs   (    0.18 ms per token,  5665.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     540.42 ms /     2 tokens (  270.21 ms per token,     3.70 tokens per second)\n",
            "llama_print_timings:        eval time =  141783.40 ms /   441 runs   (  321.50 ms per token,     3.11 tokens per second)\n",
            "llama_print_timings:       total time =  142568.28 ms /   443 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/Mozilla/gemma-3-1b-it-llamafile/resolve/main/google_gemma-3-1b-it-Q4_K_M.llamafile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXqunGGS1Ma8",
        "outputId": "7b2315dc-88ec-4162-b43a-cd83bf598115"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 23:06:41--  https://huggingface.co/Mozilla/gemma-3-1b-it-llamafile/resolve/main/google_gemma-3-1b-it-Q4_K_M.llamafile\n",
            "Resolving huggingface.co (huggingface.co)... 3.168.73.38, 3.168.73.129, 3.168.73.106, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.168.73.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/33/05/3305191594b32e502090101b11d2e0c27324a61c9509d979dbfd365d237e9984/d50ca5db210199417ee669219c71dfac83efe2bd449e3891e7d83bf7798e3ffe?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27google_gemma-3-1b-it-Q4_K_M.llamafile%3B+filename%3D%22google_gemma-3-1b-it-Q4_K_M.llamafile%22%3B&Expires=1744848401&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDg0ODQwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzMzLzA1LzMzMDUxOTE1OTRiMzJlNTAyMDkwMTAxYjExZDJlMGMyNzMyNGE2MWM5NTA5ZDk3OWRiZmQzNjVkMjM3ZTk5ODQvZDUwY2E1ZGIyMTAxOTk0MTdlZTY2OTIxOWM3MWRmYWM4M2VmZTJiZDQ0OWUzODkxZTdkODNiZjc3OThlM2ZmZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=tkwHk-aShqfRoun6kv6lOAxUDhoUd-SSc0BvuhMQQmLYm2K0BOh6McfhnxM2Xy6bFEmsCIRIataTuTLiAKxxPtKhflpvCm-QI6zCsSUWFHERKgdTtsAU%7EMkpJq-Pr0VjxPcrMxVo9aygsx-kKry9YxmFIe9NlDycbGHGiB4o0a0ndGqWyjbOxnB2Zzm9pQOOCNALal-adoWR2rdYXOEnzBlw8hicUtiPyRJVaqcXdllpgAQ1GmtwBt1NJVNX7dS8fMi3X1rNtRnJOrMJPzF4E4QgTxicMO37irNp-%7EZev3VcDUkOfsVCbZh3Vwzx5oMeTBniMFmAaUTVA0QU31KffA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-16 23:06:41--  https://cdn-lfs-us-1.hf.co/repos/33/05/3305191594b32e502090101b11d2e0c27324a61c9509d979dbfd365d237e9984/d50ca5db210199417ee669219c71dfac83efe2bd449e3891e7d83bf7798e3ffe?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27google_gemma-3-1b-it-Q4_K_M.llamafile%3B+filename%3D%22google_gemma-3-1b-it-Q4_K_M.llamafile%22%3B&Expires=1744848401&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDg0ODQwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzMzLzA1LzMzMDUxOTE1OTRiMzJlNTAyMDkwMTAxYjExZDJlMGMyNzMyNGE2MWM5NTA5ZDk3OWRiZmQzNjVkMjM3ZTk5ODQvZDUwY2E1ZGIyMTAxOTk0MTdlZTY2OTIxOWM3MWRmYWM4M2VmZTJiZDQ0OWUzODkxZTdkODNiZjc3OThlM2ZmZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=tkwHk-aShqfRoun6kv6lOAxUDhoUd-SSc0BvuhMQQmLYm2K0BOh6McfhnxM2Xy6bFEmsCIRIataTuTLiAKxxPtKhflpvCm-QI6zCsSUWFHERKgdTtsAU%7EMkpJq-Pr0VjxPcrMxVo9aygsx-kKry9YxmFIe9NlDycbGHGiB4o0a0ndGqWyjbOxnB2Zzm9pQOOCNALal-adoWR2rdYXOEnzBlw8hicUtiPyRJVaqcXdllpgAQ1GmtwBt1NJVNX7dS8fMi3X1rNtRnJOrMJPzF4E4QgTxicMO37irNp-%7EZev3VcDUkOfsVCbZh3Vwzx5oMeTBniMFmAaUTVA0QU31KffA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 108.138.128.54, 108.138.128.53, 108.138.128.37, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|108.138.128.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1113806875 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘google_gemma-3-1b-it-Q4_K_M.llamafile’\n",
            "\n",
            "google_gemma-3-1b-i 100%[===================>]   1.04G  47.8MB/s    in 21s     \n",
            "\n",
            "2025-04-16 23:07:03 (50.1 MB/s) - ‘google_gemma-3-1b-it-Q4_K_M.llamafile’ saved [1113806875/1113806875]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./google_gemma-3-1b-it-Q4_K_M.llamafile -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgqh40IP2sPW",
        "outputId": "bd644a5b-0fd7-4ea8-9ada-840eb2901c15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./google_gemma-3-1b-it-Q4_K_M.llamafile: Permission denied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llamafile-0.9.2 -m /content/google_gemma-3-1b-it-Q4_K_M.llamafile -p hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od9YaFGU20Z9",
        "outputId": "09e2eecf-2326-421e-e94d-6f6c21e891b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "note: if you have an AMD or NVIDIA GPU then you need to pass -ngl 9999 to enable GPU offloading\n",
            "Log start\n",
            "main: build = 1500 (a30b324)\n",
            "main: built with cosmocc (GCC) 11.2.0 for x86_64-linux-cosmo\n",
            "main: seed  = 1744844900\n",
            "llama_model_loader: loaded meta data with 42 key-value pairs and 340 tensors from /content/google_gemma-3-1b-it-Q4_K_M.llamafile (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gemma 3 1b It\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = it\n",
            "llama_model_loader: - kv   4:                           general.basename str              = gemma-3\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
            "llama_model_loader: - kv   6:                            general.license str              = gemma\n",
            "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 1b Pt\n",
            "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\n",
            "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\n",
            "llama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
            "llama_model_loader: - kv  12:                      gemma3.context_length u32              = 32768\n",
            "llama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 1152\n",
            "llama_model_loader: - kv  14:                         gemma3.block_count u32              = 26\n",
            "llama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 6912\n",
            "llama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 4\n",
            "llama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 512\n",
            "llama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\n",
            "llama_model_loader: - kv  35:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  36:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  37:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  38:                      quantize.imatrix.file str              = /models_out/gemma-3-1b-it-GGUF/google...\n",
            "llama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  40:             quantize.imatrix.entries_count i32              = 182\n",
            "llama_model_loader: - kv  41:              quantize.imatrix.chunks_count i32              = 129\n",
            "llama_model_loader: - type  f32:  157 tensors\n",
            "llama_model_loader: - type q5_0:  117 tensors\n",
            "llama_model_loader: - type q8_0:   14 tensors\n",
            "llama_model_loader: - type q4_K:   39 tensors\n",
            "llama_model_loader: - type q6_K:   13 tensors\n",
            "llm_load_vocab: special tokens cache size = 6414\n",
            "llm_load_vocab: token to piece cache size = 1.9446 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma3\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 262144\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 1152\n",
            "llm_load_print_meta: n_layer          = 26\n",
            "llm_load_print_meta: n_head           = 4\n",
            "llm_load_print_meta: n_head_kv        = 1\n",
            "llm_load_print_meta: n_rot            = 256\n",
            "llm_load_print_meta: n_swa            = 512\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 6912\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 999.89 M\n",
            "llm_load_print_meta: model size       = 762.49 MiB (6.40 BPW) \n",
            "llm_load_print_meta: general.name     = Gemma 3 1b It\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 248 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 106 '<end_of_turn>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
            "\u001b[Kllm_load_tensors:        CPU buffer size =   762.49 MiB\n",
            ".............................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 2048\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   208.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  208.00 MiB, K (f16):  104.00 MiB, V (f16):  104.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     1.00 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   514.25 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1047\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 1 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "\n",
            "hi what's the best way to get a new dog?\n",
            "\n",
            "Okay, let's break down the best way to get a new dog. It’s a big decision, so let’s go step-by-step!\n",
            "\n",
            "**1. Self-Reflection - Are You Ready?**\n",
            "\n",
            "*   **Time Commitment:** Dogs require *a lot* of time. Daily walks, feeding, training, playtime, grooming, vet visits – it's a significant investment. Can you realistically dedicate this time?\n",
            "*   **Financial Commitment:**  Food, vet care (annual checkups, potential emergencies), toys, training classes, grooming, and potential boarding/dog walking can add up quickly.\n",
            "*   **Lifestyle Compatibility:** Does your lifestyle fit a dog’s needs?  Do you travel frequently? Are you active or more laid-back?  A high-energy dog needs a lot more space and exercise.\n",
            "*   **Living Situation:** Do you live in an apartment or have a yard?  A large yard is ideal for a dog, but not always feasible.\n",
            "*   **Experience Level:** Are you a complete beginner, or have you had pets before?  Some breeds are easier to train and handle for first-time owners.\n",
            "\n",
            "**2. Research and Breed Selection**\n",
            "\n",
            "*   **Breed Research:**  Once you know your lifestyle, start researching breeds that might be a good fit. Websites like:\n",
            "    *   **American Kennel Club (AKC):** [https://www.akc.org/](https://www.akc.org/) – Excellent resources for breed information.\n",
            "    *   **Petfinder:** [https://www.petfinder.com/](https://www.petfinder.com/) – Browse dogs by breed and location.\n",
            "    *   **Breed-specific websites:**  Many breeds have dedicated websites with detailed information.\n",
            "*   **Considerations:**\n",
            "    *   **Energy Level:**  High-energy breeds need more exercise.\n",
            "    *   **Size:**  How much space do you have?\n",
            "    *   **Temperament:**  Consider the dog's personality – are they friendly, shy, independent?\n",
            "    *   **Grooming Needs:**  Some breeds require extensive grooming.\n",
            "*   **Don't Overthink It:**  It's okay to start with a smaller, more manageable breed if you’re a beginner.\n",
            "\n",
            "**3. Finding a Reputable Shelter or Breeder**\n",
            "\n",
            "*   **Animal Shelters:** This is *always* the best first option!  You’re giving a dog a second chance at a loving home.  Shelters are often full of wonderful dogs who need a good home.\n",
            "    *   **Benefits:** Lower adoption fees, often already vaccinated and spayed/neutered, and you're giving the dog a chance to become part\n",
            "\n",
            "llama_print_timings:        load time =     433.12 ms\n",
            "llama_print_timings:      sample time =     210.27 ms /   593 runs   (    0.35 ms per token,  2820.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     169.64 ms /     2 tokens (   84.82 ms per token,    11.79 tokens per second)\n",
            "llama_print_timings:        eval time =   89147.93 ms /   592 runs   (  150.59 ms per token,     6.64 tokens per second)\n",
            "llama_print_timings:       total time =   89931.83 ms /   594 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llamafile-0.9.2.1 -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5urnG3h3EVm",
        "outputId": "d58f4e7e-6598-49d7-c37c-b21037695163"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./llamafile-0.9.2.1: Permission denied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x llamafile-0.9.2.1"
      ],
      "metadata": {
        "id": "FqwyVe7W3epx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./llamafile-0.9.2.1 -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AxxWi-CW3ltt",
        "outputId": "c3b0fbe1-5132-403d-890f-b48177f1bb3e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b7\u001b[?47h\u001b[?1h\u001b=\r\u001b[4mLLAMAFILE\u001b[24m(1)                General Commands Manual               \u001b[4mLLAMAFILE\u001b[24m(1)\u001b[m\r\n",
            "\u001b[m\r\n",
            "\u001b[1mNAME\u001b[0m\u001b[m\r\n",
            "       llamafile — large language model runner\u001b[m\r\n",
            "\u001b[m\r\n",
            "\u001b[1mSYNOPSIS\u001b[0m\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--chat\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[0m\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--server\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m [\u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m]\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m \u001b[1m--image\u001b[0m\u001b[m\r\n",
            "                 \u001b[4mgraphic.png\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\r\n",
            "\u001b[m\r\n",
            "\u001b[1mDESCRIPTION\u001b[0m\u001b[m\r\n",
            "       \u001b[1mllamafile \u001b[22mis a large language model tool. It has use cases such as:\u001b[m\r\n",
            "\u001b[m\r\n",
            "       \u001b[1m-   \u001b[22mCode completion\u001b[m\r\n",
            "       \u001b[1m-   \u001b[22mProse composition\u001b[m\r\n",
            "       \u001b[1m-   \u001b[22mChatbot that passes the Turing test\u001b[m\r\n",
            "       \u001b[1m-   \u001b[22mText/image summarization and analysis\u001b[m\r\n",
            "\u001b[m\r\n",
            "\u001b[1mMODES\u001b[0m\u001b[m\r\n",
            "       There's three modes of operation: \u001b[1m--chat\u001b[22m, \u001b[1m--server\u001b[22m, and \u001b[1m--cli\u001b[22m.  If none\u001b[m\r\n",
            "       of  these flags is specified, then llamafile makes its best guess about\u001b[m\n",
            "\u001b[7m/tmp/paginate.v8g6fn\u001b[m\u001b[K\u0007\u001b[H\u001b[2J\u001b[H\u001b[H\u001b[2J\u001b[H\u001b[4mLLAMAFILE\u001b[24m(1)                General Commands Manual               \u001b[4mLLAMAFILE\u001b[24m(1)\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mNAME\u001b[0m\u001b[m\n",
            "       llamafile — large language model runner\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mSYNOPSIS\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--chat\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--server\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m [\u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m]\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m \u001b[1m--image\u001b[0m\u001b[m\n",
            "                 \u001b[4mgraphic.png\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mDESCRIPTION\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22mis a large language model tool. It has use cases such as:\u001b[m\n",
            "\u001b[m\n",
            "       \u001b[1m-   \u001b[22mCode completion\u001b[m\n",
            "       \u001b[1m-   \u001b[22mProse composition\u001b[m\n",
            "       \u001b[1m-   \u001b[22mChatbot that passes the Turing test\u001b[m\n",
            "       \u001b[1m-   \u001b[22mText/image summarization and analysis\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mMODES\u001b[0m\u001b[m\n",
            "       There's three modes of operation: \u001b[1m--chat\u001b[22m, \u001b[1m--server\u001b[22m, and \u001b[1m--cli\u001b[22m.  If none\u001b[m\n",
            "       of  these flags is specified, then llamafile makes its best guess about\u001b[m\n",
            ":\u001b[K\u0007\u001b[H\u001b[2J\u001b[H\u001b[H\u001b[2J\u001b[H\u001b[4mLLAMAFILE\u001b[24m(1)                General Commands Manual               \u001b[4mLLAMAFILE\u001b[24m(1)\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mNAME\u001b[0m\u001b[m\n",
            "       llamafile — large language model runner\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mSYNOPSIS\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--chat\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--server\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m [\u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m]\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m \u001b[1m--image\u001b[0m\u001b[m\n",
            "                 \u001b[4mgraphic.png\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mDESCRIPTION\u001b[0m\u001b[m\n",
            "       \u001b[1mllamafile \u001b[22mis a large language model tool. It has use cases such as:\u001b[m\n",
            "\u001b[m\n",
            "       \u001b[1m-   \u001b[22mCode completion\u001b[m\n",
            "       \u001b[1m-   \u001b[22mProse composition\u001b[m\n",
            "       \u001b[1m-   \u001b[22mChatbot that passes the Turing test\u001b[m\n",
            "       \u001b[1m-   \u001b[22mText/image summarization and analysis\u001b[m\n",
            "\u001b[m\n",
            "\u001b[1mMODES\u001b[0m\u001b[m\n",
            "       There's three modes of operation: \u001b[1m--chat\u001b[22m, \u001b[1m--server\u001b[22m, and \u001b[1m--cli\u001b[22m.  If none\u001b[m\n",
            "       of  these flags is specified, then llamafile makes its best guess about\u001b[m\n",
            ":\u001b[K"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7b6db621e66f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./llamafile-0.9.2.1 -h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oMBYKyii3oM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}